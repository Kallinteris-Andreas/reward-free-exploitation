import gymnasium
import numpy as np
import copy
import os
import argparse
import cloudpickle
import torch
import mujoco

import stable_baselines3
from stable_baselines3 import TD3, PPO, A2C, SAC, DQN
from stable_baselines3.common.logger import configure
# from stable_baselines3.common.callbacks import EvalCallback
from my_eval import EvalCallback
from gymnasium.wrappers import TransformReward, PassiveEnvChecker, OrderEnforcing, TimeLimit
from stable_baselines3.common.monitor import Monitor
import cartpole_fixed
import cartpole_sutton_reward
import cartpole_dumb_reward
import humanoid_v5_ahr

assert np.__version__ == "1.26.2"
assert gymnasium.__version__ == "0.29.0"
assert cloudpickle.__version__ == "3.0.0"
assert torch.__version__ == "2.2.1+cu118"
assert stable_baselines3.__version__ == "2.2.1"
assert mujoco.__version__ == "2.3.7"


def make_env(env_id: str):
    if env_id == "CartPole-v1-fixed":
        return TimeLimit(OrderEnforcing(PassiveEnvChecker(cartpole_fixed.CartPoleEnv())), max_episode_steps=500)
    elif env_id == "CartPole-v1_sutton_reward":
        return TimeLimit(OrderEnforcing(PassiveEnvChecker(cartpole_sutton_reward.CartPoleEnv())), max_episode_steps=500)
    elif env_id == "CartPole-v1_dumb_reward":
        return TimeLimit(OrderEnforcing(PassiveEnvChecker(cartpole_dumb_reward.CartPoleEnv())), max_episode_steps=500)
    elif env_id == "HumanoidPendulum-v5":
        return gymnasium.make("Humanoid-v5", forward_reward_weight=0, ctrl_cost_weight=0.2, contact_cost_weight=0, include_cinert_in_observation=False, include_cvel_in_observation=False, include_qfrc_actuator_in_observation=False, include_cfrc_ext_in_observation=False)
    elif env_id == "HumanoidPendulum-v5ahr":
        return TimeLimit(OrderEnforcing(PassiveEnvChecker(humanoid_v5_ahr.HumanoidEnv(forward_reward_weight=0, ctrl_cost_weight=0.2, contact_cost_weight=0, include_cinert_in_observation=False, include_cvel_in_observation=False, include_qfrc_actuator_in_observation=False, include_cfrc_ext_in_observation=False))), max_episode_steps=1000)
    elif env_id == "Humanoid-v5ahr":
        return TimeLimit(OrderEnforcing(PassiveEnvChecker(humanoid_v5_ahr.HumanoidEnv())), max_episode_steps=1000)
    elif env_id == "AntPendulum-v5":
        return gymnasium.make("Ant-v5", forward_reward_weight=0, ctrl_cost_weight=1.0, contact_cost_weight=0, include_cfrc_ext_in_observation=False)
    else:
        return gymnasium.make(env_id, render_mode=None)


def make_model(algorithm: str):
    match args.algo:
        case "TD3":  # note does not work with Discrete
            return TD3("MlpPolicy", env, seed=run, verbose=1, device='cuda', learning_starts=100)
        case "PPO":
            return PPO("MlpPolicy", env, seed=run, verbose=1, device='cuda')
        case "SAC":  # note does not work with Discrete
            return SAC("MlpPolicy", env, seed=run, verbose=1, device='cuda', learning_starts=100)
        case "A2C":
            return A2C("MlpPolicy", env, seed=run, verbose=1, device='cuda')
        case "DQN":
            return DQN("MlpPolicy", env, seed=run, verbose=1, device='cuda', learning_starts=100)


parser = argparse.ArgumentParser()
parser.add_argument("--algo", default="DQN")
parser.add_argument("--env_id", default="CartPole-v1")
parser.add_argument("--eval_env_id", default=None)
parser.add_argument("--starting_run", default=0, type=int)
args = parser.parse_args()

RUNS = 30  # Number of Statistical Runs
TOTAL_TIME_STEPS = 1_000_000
EVAL_SEED = 1234
EVAL_FREQ = 500
EVAL_ENVS = 50


for run in range(args.starting_run, RUNS):
    env = Monitor(make_env(args.env_id))
    if args.eval_env_id is None:
        eval_env = copy.deepcopy(env)
    else:
        eval_env = Monitor(make_env(args.eval_env_id))
    eval_path = f"results/{args.env_id}/{args.algo}/run_" + str(run)

    assert not os.path.exists(eval_path)

    #eval_callback = EvalCallback(eval_env, best_model_save_path=eval_path, log_path=eval_path, n_eval_episodes=EVAL_ENVS, eval_freq=EVAL_FREQ, deterministic=True, render=False, verbose=True)
    eval_callback = EvalCallback(eval_env, best_model_save_path=eval_path, log_path=eval_path, n_eval_episodes=EVAL_ENVS, eval_freq=EVAL_FREQ, deterministic=True, render=False, verbose=True, seed=EVAL_SEED)

    model = make_model(args.algo)
    # model.set_logger(configure(eval_path, ["stdout", "csv"]))
    model.set_logger(configure(eval_path, ["csv"]))

    model.learn(total_timesteps=TOTAL_TIME_STEPS, callback=eval_callback)
    print(f"Finished run: {run}")
