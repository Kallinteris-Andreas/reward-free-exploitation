import gymnasium as gym
import gymnasium
from gymnasium.experimental.wrappers import RescaleActionV0
import time
import argparse
from stable_baselines3.common.vec_env import VecVideoRecorder

import numpy as np

from stable_baselines3 import TD3, PPO, A2C, SAC
from stable_baselines3.common.logger import configure
from stable_baselines3.common.evaluation import evaluate_policy


parser = argparse.ArgumentParser()
parser.add_argument("--mode", default="render")
parser.add_argument("--model")
args = parser.parse_args()

match args.mode:
    case "render":
        RENDER_MODE = "human"
    case "info":
        RENDER_MODE = "rgb_array"
    case "eval":
        RENDER_MODE = "rgb_array"
    case "video":
        RENDER_MODE = "rgb_array"

eval_env = gymnasium.make("Humanoid-v5", render_mode=RENDER_MODE, forward_reward_weight=0, ctrl_cost_weight=0.2, contact_cost_weight=0, include_cinert_in_observation=False, include_cvel_in_observation=False, include_qfrc_actuator_in_observation=False, include_cfrc_ext_in_observation=False)
# eval_env = RescaleActionV0(eval_env, min_action=-1, max_action=1)
 
# make model
model = SAC.load(path=f'./results/{args.model}/best_model', env=eval_env, device='cpu')


#
# RECORD VIDEO
#
if args.mode == "video":
    video_folder = "videos/"
    video_length = 1000
    VIDEO_NAME = args.model.replace("/", "_")
    vec_env = VecVideoRecorder(model.get_env(), video_folder, record_video_trigger=lambda x: x == 0, video_length=video_length, name_prefix=f"{VIDEO_NAME}")
    obs = vec_env.reset()
    for _ in range(video_length + 1):
        action, _state = model.predict(obs, deterministic=True)
        obs, _, _, _ = vec_env.step(action)
    # Save the video
    vec_env.close()


#
# Evaluate Policy
#

if args.mode == "eval":
    avg_return, std_return = evaluate_policy(model, eval_env, n_eval_episodes=1000)
    print(f"the average return is {avg_return}")



#
# Render Human
#
STEPS=10000
if args.mode in ["render", "info"]:
    vec_env = model.get_env()
    obs = vec_env.reset()
    infos = []
    for step in range(STEPS):
        action, _state = model.predict(obs, deterministic=True)
        obs, reward, done, info = vec_env.step(action)
        #print(action)
        #print(info)
        infos.append(info)
        if args.mode == "render":
            time.sleep(0.100)

    print(f"reward_foward = {sum([info[0]['reward_forward']for info in infos])/STEPS}")
    print(f"reward_ctrl = {sum([info[0]['reward_ctrl']for info in infos])/STEPS}")
    print(f"reward_contact = {sum([info[0]['reward_contact']for info in infos])/STEPS}")
    print(f"reward_survive = {sum([info[0]['reward_survive']for info in infos])/STEPS}")
